{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ghlHAH0Nlhi"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "If you are running on Google Colab, you need to install the necessary dependencies before beginning the exercise. \n",
        "\n",
        "**Note:** Don't forget to restart the environment with the button in the output. Some packages are already imported and need to be re-imported after installing the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHNPcnXPNlhj",
        "outputId": "841380d6-9fe1-44d9-8e12-e8f3a050598d"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb python-opengl ffmpeg cmake\n",
        "\n",
        "!pip uninstall -y pyarrow\n",
        "!pip install ray[rllib] gym==0.21 pyvirtualdisplay bs4 ez_setup pygame\n",
        "!pip install --upgrade setuptools\n",
        "\n",
        "print(\"Successfully installed all the dependencies!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezI9SokvcC8N"
      },
      "source": [
        "# Import libraries and define virtual display to display video in Colab\n",
        "\n",
        "Utility functions that enable recording and displaying a video of the environment in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RqXJ5-U5JAB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whZ08tZmdI0J"
      },
      "source": [
        "# Define CartPoleRightLeft environment\n",
        "\n",
        "In this section we define the custom CartPoleRightLeft environment and register it with rllib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vd-jSf8m_j0"
      },
      "source": [
        "#### Inspection Tool\n",
        "\n",
        "We can use this code snippet to see the original implementation of environment functions to make necessary changes. (This can also be done by looking at the source code on Github, etc but using this tool we're sure about the version we are extending.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bjtmr4YOuc7e"
      },
      "outputs": [],
      "source": [
        "# Helper code snippet to look into original implementation of different functions in the original env class\n",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "\n",
        "import inspect\n",
        "env = CartPoleEnv()\n",
        "lines = inspect.getsource(env.reset)\n",
        "print(lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_HLGA3nDWt"
      },
      "source": [
        "#### CartPoleRightLeft\n",
        "\n",
        "In this cell, we create the custom environment for the CartPoleRightLeft enviroment.\n",
        "\n",
        "We want the agent to balance the pole on the left or right side of the screen depending on a state variable provided by the environment. We name this variable `directional_goal`. To add this new state variable we perform the following changes in the environment:\n",
        "\n",
        "*   Add the new variable to the `self.observation_space` attribute. (*lines 103-106*) \n",
        "*   Add the variable to the `reset` function. (*lines 199-202*)\n",
        "*   Use the variable in the `step` function to calculate reward. Detailed below.\n",
        "\n",
        "*(Holding Ctrl and pressing M L (one by one) switches on/off line numbers in the cells containing code.)*\n",
        "\n",
        "The next step is to define a new reward function that takes the `direction_goal` variable into account. To achieve this we assume that the agent receives maximum reward when it is half its width away from the edge of the screen. We use the inspection tool above along very basic algebra to calculate the distances and the position of maximum reward. (*line 108-116*)\n",
        "\n",
        "This is illustrated in the image below:\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=11TaxSwO4I7DIcvRbBs6BpMkwoHbblyoZ\n",
        ")\n",
        "\n",
        "Finally, we modify the `step` function and add this positional reward to the existing reward. The reward is setup in a way that the agent recieves a reward of `1` if it is balancing the pole in one of the edges. Moreover, the reward linearly decreases the further the agent is from the respective edge. The scaling factor is setup in a way that the agent gets a reward of `0.5` when it is in the middle position. (*lines 118-135, 167, 176-181*)\n",
        "\n",
        "Note that the reward must be designed in way that that the agent still has incentive to balance the pole while also being incentivized to move towards one of the edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb3twh0pdLOR"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Classic cart-pole system implemented by Rich Sutton et al.\n",
        "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
        "permalink: https://perma.cc/C9ZM-652R\n",
        "\"\"\"\n",
        "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "\n",
        "import math\n",
        "from typing import Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "from gym import logger, spaces\n",
        "from gym.error import DependencyNotInstalled\n",
        "\n",
        "class CartPoleRightLeft(CartPoleEnv):\n",
        "    \"\"\"\n",
        "    ### Description\n",
        "\n",
        "    This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson in\n",
        "    [\"Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem\"](https://ieeexplore.ieee.org/document/6313077).\n",
        "    A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "    The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces\n",
        "     in the left and right direction on the cart.\n",
        "\n",
        "    ### Action Space\n",
        "\n",
        "    The action is a `ndarray` with shape `(1,)` which can take values `{0, 1}` indicating the direction\n",
        "     of the fixed force the cart is pushed with.\n",
        "\n",
        "    | Num | Action                 |\n",
        "    |-----|------------------------|\n",
        "    | 0   | Push cart to the left  |\n",
        "    | 1   | Push cart to the right |\n",
        "\n",
        "    **Note**: The velocity that is reduced or increased by the applied force is not fixed and it depends on the angle\n",
        "     the pole is pointing. The center of gravity of the pole varies the amount of energy needed to move the cart underneath it\n",
        "\n",
        "    ### Observation Space\n",
        "\n",
        "    The observation is a `ndarray` with shape `(5,)` with the values corresponding to the following positions and velocities:\n",
        "\n",
        "    | Num | Observation           | Min                 | Max               |\n",
        "    |-----|-----------------------|---------------------|-------------------|\n",
        "    | 0   | Cart Position         | -4.8                | 4.8               |\n",
        "    | 1   | Cart Velocity         | -Inf                | Inf               |\n",
        "    | 2   | Pole Angle            | ~ -0.418 rad (-24°) | ~ 0.418 rad (24°) |\n",
        "    | 3   | Pole Angular Velocity | -Inf                | Inf               |\n",
        "    | 4   | Direction Goal        | 0 (Left)            | 1 (Right)         |\n",
        "\n",
        "    **Note:** While the ranges above denote the possible values for observation space of each element,\n",
        "        it is not reflective of the allowed values of the state space in an unterminated episode. Particularly:\n",
        "    -  The cart x-position (index 0) can be take values between `(-4.8, 4.8)`, but the episode terminates\n",
        "       if the cart leaves the `(-2.4, 2.4)` range.\n",
        "    -  The pole angle can be observed between  `(-.418, .418)` radians (or **±24°**), but the episode terminates\n",
        "       if the pole angle is not in the range `(-.2617, .2617)` (or **±15°**)\n",
        "\n",
        "    ### Rewards\n",
        "\n",
        "    Since the goal is to keep the pole upright for as long as possible, a reward of `+1` for every step taken,\n",
        "    including the termination step, is allotted. The threshold for rewards is 475 for v1.\n",
        "\n",
        "    ### Starting State\n",
        "\n",
        "    All observations except the direction goal are assigned a uniformly random value in `(-0.05, 0.05)`.\n",
        "    The direction goal is randomly chosen between 0 and 1.\n",
        "\n",
        "    ### Episode Termination\n",
        "\n",
        "    The episode terminates if any one of the following occurs:\n",
        "    1. Pole Angle is greater than ±12°\n",
        "    2. Cart Position is greater than ±2.4 (center of the cart reaches the edge of the display)\n",
        "    3. Episode length is greater than 500 (200 for v0)\n",
        "\n",
        "    ### Arguments\n",
        "\n",
        "    ```\n",
        "    gym.make('CartPole-v1')\n",
        "    ```\n",
        "\n",
        "    No additional arguments are currently supported.\n",
        "    \"\"\"\n",
        "    def __init__(self, render_mode: Optional[str] = None):\n",
        "        super(CartPoleRightLeft, self).__init__()\n",
        "\n",
        "        self.screen_width = 600\n",
        "        self.screen_height = 400\n",
        "\n",
        "        # Angle limit set to 2 * theta_threshold_radians so failing observation\n",
        "        # is still within bounds.\n",
        "        high = np.array(\n",
        "            [\n",
        "                self.x_threshold * 2,\n",
        "                np.finfo(np.float32).max,\n",
        "                self.theta_threshold_radians * 2,\n",
        "                np.finfo(np.float32).max,\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        self.observation_space = spaces.Box(\n",
        "            np.append(-high, 0), \n",
        "            np.append(high, 1), dtype=np.float32\n",
        "            )\n",
        "\n",
        "        # width of cart not scaled to screen\n",
        "        self.cart_width = 50.0\n",
        "        world_width = self.x_threshold * 2\n",
        "        scale = self.screen_width / world_width\n",
        "        self.unscaled_cart_width = self.cart_width / scale\n",
        "\n",
        "        # absolute position of the maximum reward position\n",
        "        self.maximum_reward_position = \\\n",
        "          self.x_threshold - self.unscaled_cart_width / 2\n",
        "\n",
        "        # positional_reward_scaling_factor\n",
        "        self.reward_scaling_factor = \\\n",
        "          self.x_threshold * 2 - (self.unscaled_cart_width / 2)\n",
        "\n",
        "    def calculate_positional_reward(self, x, direction_goal):\n",
        "      \"\"\"\n",
        "      Method that takes in the x position of the cart alongside the \n",
        "      direction goal and calculates the positional reward of the agent.\n",
        "\n",
        "      The positional reward is setup in a way that the agent gets a less total\n",
        "      reward if the cart is far from the maximum reward position.\n",
        "      \"\"\"\n",
        "      if direction_goal == 0:\n",
        "        maximum_reward_position = -self.maximum_reward_position\n",
        "      else:\n",
        "        maximum_reward_position = self.maximum_reward_position\n",
        "\n",
        "      return (-abs(x - maximum_reward_position)) / self.reward_scaling_factor\n",
        "\n",
        "    def step(self, action):\n",
        "        err_msg = f\"{action!r} ({type(action)}) invalid\"\n",
        "        assert self.action_space.contains(action), err_msg\n",
        "        assert self.state is not None, \"Call reset before using step method.\"\n",
        "        x, x_dot, theta, theta_dot, direction_goal = self.state\n",
        "        force = self.force_mag if action == 1 else -self.force_mag\n",
        "        costheta = math.cos(theta)\n",
        "        sintheta = math.sin(theta)\n",
        "\n",
        "        # For the interested reader:\n",
        "        # https://coneural.org/florian/papers/05_cart_pole.pdf\n",
        "        temp = (\n",
        "            force + self.polemass_length * theta_dot**2 * sintheta\n",
        "        ) / self.total_mass\n",
        "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
        "            self.length * (4.0 / 3.0 - self.masspole * costheta**2 / self.total_mass)\n",
        "        )\n",
        "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
        "\n",
        "        if self.kinematics_integrator == \"euler\":\n",
        "            x = x + self.tau * x_dot\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "        else:  # semi-implicit euler\n",
        "            x_dot = x_dot + self.tau * xacc\n",
        "            x = x + self.tau * x_dot\n",
        "            theta_dot = theta_dot + self.tau * thetaacc\n",
        "            theta = theta + self.tau * theta_dot\n",
        "\n",
        "        self.state = (x, x_dot, theta, theta_dot, direction_goal)\n",
        "\n",
        "        done = bool(\n",
        "            x < -self.x_threshold\n",
        "            or x > self.x_threshold\n",
        "            or theta < -self.theta_threshold_radians\n",
        "            or theta > self.theta_threshold_radians\n",
        "        )\n",
        "\n",
        "        if not done:\n",
        "            reward = 1.0 + self.calculate_positional_reward(x, direction_goal)\n",
        "        elif self.steps_beyond_done is None:\n",
        "            # Pole just fell!\n",
        "            self.steps_beyond_done = 0\n",
        "            reward = 1.0 + self.calculate_positional_reward(x, direction_goal)\n",
        "        else:\n",
        "            if self.steps_beyond_done == 0:\n",
        "                logger.warn(\n",
        "                    \"You are calling 'step()' even though this \"\n",
        "                    \"environment has already returned done = True. You \"\n",
        "                    \"should always call 'reset()' once you receive 'done = \"\n",
        "                    \"True' -- any further steps are undefined behavior.\"\n",
        "                )\n",
        "            self.steps_beyond_done += 1\n",
        "            reward = 0.0\n",
        "\n",
        "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "    ):\n",
        "        direction_goal = np.random.randint(2)\n",
        "        self.state = np.append(\n",
        "            self.np_random.uniform(low=-0.05, high=0.05, size=(4,)), \n",
        "            direction_goal\n",
        "            )\n",
        "        self.steps_beyond_done = None\n",
        "        return np.array(self.state, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZjqR3kpQslK"
      },
      "source": [
        "#### Register CartPoleRightLeft as an Rllib environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTclEUVWQwDq"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "from ray.rllib.models import ModelCatalog\n",
        "from ray.tune.registry import register_env\n",
        "\n",
        "env_name = 'CartPoleRightLeft'\n",
        "register_env(env_name, lambda config: CartPoleRightLeft())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQJq2wvZ9vpS"
      },
      "source": [
        "# Test CartPoleRightLeft and virtual display by performing random actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JLHOLuYrGWl"
      },
      "outputs": [],
      "source": [
        "# env = wrap_env(CartPoleRightLeft())\n",
        "# env.reset()\n",
        "# for _ in range(20):\n",
        "#     action = env.action_space.sample()\n",
        "#     observation, reward, done, info = env.step(action)\n",
        "\n",
        "#     env.render()\n",
        "\n",
        "#     # print(observation, reward, done)\n",
        "#     if done:\n",
        "#         env.reset()\n",
        "# env.close()\n",
        "# show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv9tv2S0kiSU"
      },
      "source": [
        "# Training\n",
        "\n",
        "We use the `ray.tune` API to train the agent with different algorithms. We try out two well-known algorithms which work well on continous state space and discrete action space. The different config variables for the two algorithms can be seen below.\n",
        "\n",
        "We train the algorithms with limiting the training iterations and limiting the timesteps to compare the two algorithms efficiency as well as their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXQ8hIB9Nlh0"
      },
      "source": [
        "#### PPO\n",
        "\n",
        "https://docs.ray.io/en/releases-1.3.0/rllib-algorithms.html#proximal-policy-optimization-ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok210MCfNlh5"
      },
      "outputs": [],
      "source": [
        "from ray import tune\n",
        "from ray.rllib.agents.ppo import DEFAULT_CONFIG as PPO_DEFAULT\n",
        "\n",
        "ppo_config = PPO_DEFAULT.copy()\n",
        "ppo_config['framework'] = 'torch'\n",
        "# ppo_config['evaluation_interval'] = 10\n",
        "ppo_config['env'] = 'CartPoleRightLeft'\n",
        "ppo_config['num_workers'] = 1\n",
        "ppo_config['num_sgd_iter'] = tune.grid_search([15, 30, 45, 60])\n",
        "ppo_config['sgd_minibatch_size'] = tune.grid_search([500])\n",
        "ppo_config['model']['fcnet_hiddens'] = [50, 50]\n",
        "ppo_config['evaluation_num_workers'] = 0\n",
        "ppo_config['evaluation_config']['render_env'] = False\n",
        "ppo_config['num_gpus'] = 0\n",
        "ppo_config['evaluation_duration'] = 1000\n",
        "ppo_config['evaluation_duration_unit'] = 'timesteps'\n",
        "ppo_config['horizon'] = 1000\n",
        "\n",
        "ppo_agent = \"PPO\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pPnwjLQrXIl"
      },
      "source": [
        "#### DQN\n",
        "\n",
        "https://docs.ray.io/en/releases-1.3.0/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64i8RvuErZLs"
      },
      "outputs": [],
      "source": [
        "from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT\n",
        "\n",
        "dqn_config = DQN_DEFAULT.copy()\n",
        "dqn_config['framework'] = 'torch'\n",
        "# dqn_config['evaluation_interval'] = 10\n",
        "dqn_config['env'] = 'CartPoleRightLeft'\n",
        "dqn_config['num_workers'] = 1\n",
        "dqn_config['model']['fcnet_hiddens'] = [50, 50]\n",
        "dqn_config['evaluation_config']['render_env'] = False\n",
        "dqn_config['lr'] = tune.grid_search([0.01, 0.001, 0.0001])\n",
        "dqn_config['num_gpus'] = 0\n",
        "dqn_config['evaluation_duration'] = 1000\n",
        "dqn_config['evaluation_duration_unit'] = 'timesteps'\n",
        "dqn_config['horizon'] = 1000\n",
        "\n",
        "dqn_agent = \"DQN\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tARo6h2eqrIm"
      },
      "source": [
        "#### Train with ray tune\n",
        "\n",
        "Change the `trainer_config` and `trainer_algorithm` variables based on the algorithm you want to try out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mD94gkOup_mt",
        "outputId": "307c4e87-4129-4fbf-961a-9fff6a55ffef"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import gym\n",
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.logger import pretty_print\n",
        "\n",
        "# Select algorithm to use.\n",
        "trainer_config = ppo_config\n",
        "trainer_algorithm = ppo_agent\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True)\n",
        "register_env(env_name, lambda config: CartPoleRightLeft())\n",
        "\n",
        "analysis = tune.run(\n",
        "    trainer_algorithm,\n",
        "    stop={\n",
        "        # \"episode_reward_mean\": 500,\n",
        "        # \"training_iteration\": 50,\n",
        "        \"timesteps_total\": 50_000,\n",
        "        # \"episodes_total\": 1000,\n",
        "        },\n",
        "    config=trainer_config,\n",
        "    checkpoint_at_end=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azVFyHlHhIfd"
      },
      "source": [
        "#### Train agent for `N_ITER` iterations manually. (Don't use) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "3o0wjdZ3Nlh7",
        "outputId": "ec2a5ee3-e54e-40c8-db43-bf9ea7892f5c"
      },
      "outputs": [],
      "source": [
        "# N_ITER = 3\n",
        "# for i in range(N_ITER):\n",
        "#     result = agent.train()\n",
        "#     print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbQM0OGDh1j-"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "For the evaluation we compare the performance of the algorithms in `500` timesteps. We analyze the `episode_len_mean` and the `episode_reward_mean`.\n",
        "\n",
        "Based on the evaluations it seems that the PPO agent is performing better and more stable. However, not that the `episode_reward_max` is higher for the DQN. At the same time, the `episode_reward_min` is lower for the DQN algorithm. This points to the DQN algorithm being more unstable and trying to go towards the edge of the screen faster.\n",
        "\n",
        "To reach more concrete conclusions we will need to perform more extensive evaluations and hyper-parameter tunings that were out of the scope of this task because of time constraints. We will also need to refine the definition of success for this task. In the current reward and evaluation setup we can't define an exact criteria for the success of the task but we can compare different methods together. A good criteria might be balancing the pole for `T` time steps in a small boundary around the maximum reward point.\n",
        "\n",
        "Finally, in the current defined environment the agent still gets positive rewards if it balances the pole in the middle of the screen. It might be worth designing a more complex reward function that penalizes the agent for balancing in the middle. (However, this is also challenging as it might motivate the agent to just drop the pole to not get negative rewards.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQJVH9tOp8pa"
      },
      "source": [
        "#### Rllib Evaluation\n",
        "\n",
        "We can use the `analysis` object to get the best checkpoint of the trained agent based on different metrics. Here we use the `episode_len_mean`. Note that since we have changed the reward function, the total reward of the agent will be lower than the original environment even if the pole is balanced for the same number of timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56v-zAmJk3q9"
      },
      "source": [
        "#### Load agent from checkpoint\n",
        "\n",
        "We use the ExperimentAnalysis class to access the best configs and checkpoints for each algorithm and in total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jReKx2ATFnAP"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray.tune.registry import register_env\n",
        "from ray.rllib.agents.ppo import PPOTrainer\n",
        "from ray.rllib.agents.dqn import DQNTrainer\n",
        "\n",
        "from ray.tune import ExperimentAnalysis\n",
        "analysis = ExperimentAnalysis(\"~/ray_results/\")\n",
        "\n",
        "best_trial = trial=analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")\n",
        "\n",
        "best_config = analysis.get_best_config(metric=\"episode_reward_mean\", mode=\"max\")\n",
        "best_config['evaluation_duration'] = 10000\n",
        "best_config['evaluation_duration_unit'] = 'timesteps'\n",
        "best_config['evaluation_num_workers'] = 1\n",
        "\n",
        "best_checkpoint = analysis.get_best_checkpoint(\n",
        "    metric=\"episode_reward_mean\", mode=\"max\", trial=best_trial\n",
        ")\n",
        "\n",
        "best_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Qq2_AYVNliE"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint_path = best_checkpoint\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(ignore_reinit_error=True)\n",
        "register_env('CartPoleRightLeft', lambda config: CartPoleRightLeft())\n",
        "\n",
        "# Load agent from checkpoint\n",
        "agent_trainer = PPOTrainer\n",
        "\n",
        "test_agent = agent_trainer(best_config, 'CartPoleRightLeft')\n",
        "test_agent.restore(checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgUlvO9MkzkE"
      },
      "source": [
        "#### Rllib Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ujVgspXWurQ"
      },
      "outputs": [],
      "source": [
        "test_agent.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRa1Qf4uh3c4"
      },
      "source": [
        "#### Run and render trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9asL5Z5lNliH"
      },
      "outputs": [],
      "source": [
        "# Run trained agent in environment for final evaluation\n",
        "\n",
        "env = wrap_env(CartPoleRightLeft())\n",
        "state = env.reset()\n",
        "done = False\n",
        "cumulative_reward = 0\n",
        "\n",
        "print(state)\n",
        "step_count = 0\n",
        "while not done and step_count < 3000:\n",
        "    action = test_agent.compute_action(state)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    step_count += 1\n",
        "\n",
        "    env.render()\n",
        "    cumulative_reward += reward\n",
        "\n",
        "print(cumulative_reward)\n",
        "env.close()\n",
        "show_video()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7ghlHAH0Nlhi",
        "ezI9SokvcC8N",
        "4vd-jSf8m_j0",
        "BB_HLGA3nDWt",
        "nZjqR3kpQslK",
        "qXQ8hIB9Nlh0",
        "3pPnwjLQrXIl",
        "tARo6h2eqrIm",
        "azVFyHlHhIfd",
        "56v-zAmJk3q9",
        "zgUlvO9MkzkE",
        "ZRa1Qf4uh3c4"
      ],
      "name": "CartPoleRightLeft",
      "provenance": []
    },
    "gpuClass": "standard",
    "hide_code_all_hidden": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
